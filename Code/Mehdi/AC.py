# -*- coding: utf-8 -*-
"""VAC_ECG_03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17NLYDvF46Cb6c1x1vdKqX8WwdWpfkknn

# Neptune Tracker
"""



"""# VAC Classe

"""

import os
import pickle
import time

from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Conv2D, Conv1D, ReLU, BatchNormalization, \
    Flatten, Dense, Reshape, Conv2DTranspose, Conv1DTranspose, Activation, Lambda
from tensorflow.keras import backend as K
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanSquaredError
import numpy as np
import tensorflow as tf

from tensorflow.keras.losses import BinaryCrossentropy

tf.compat.v1.disable_eager_execution()


def mse(y_target, y_predicted):
    error = y_target - y_predicted
    reconstruction_loss = K.mean(K.square(error), axis=[1, 2])
    return reconstruction_loss


checkpoint_filepath = 'content/check/{epoch:02d}-{val_loss:.2f}.hdf5'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_loss',
    mode='min',
    save_best_only=True)


class AE:
    """
    VAE represents a Deep Convolutional variational autoencoder architecture
    with mirrored encoder and decoder components.
    """

    def __init__(self,
                 input_shape,
                 conv_filters,
                 conv_kernels,
                 conv_strides,
                 latent_space_dim):
        self.input_shape = input_shape
        self.conv_filters = conv_filters
        self.conv_kernels = conv_kernels
        self.conv_strides = conv_strides
        self.latent_space_dim = latent_space_dim

        self.encoder = None
        self.decoder = None
        self.model = None

        self._encoder_output_dim = None
        self._num_conv_layers = len(conv_filters)
        self._model_input = None

        self._build()

    def summary(self):
        self.encoder.summary()
        self.decoder.summary()
        self.model.summary()

    def compile(self, learning_rate=0.0001):
        print('In compile.')
        optimizer = Adam(learning_rate=learning_rate)
        print('Optimizer set.')
        self.model.compile(optimizer=optimizer,
                           loss=self._calculate_reconstruction_loss,
                           metrics=[tf.keras.metrics.MeanSquaredError()])
        print('End of compile')

    # NeptuneLogger()
    def train(self, x_train, batch_size, num_epochs):
        return self.model.fit(x_train,
                              x_train,
                              validation_split=0.285,
                              callbacks=[model_checkpoint_callback],
                              batch_size=batch_size,
                              epochs=num_epochs,
                              shuffle=True)

    def train_with_avg(self, x_train, x_avg, batch_size, num_epochs):
        return self.model.fit(x_train,
                              x_avg,
                              validation_split=0.285,
                              callbacks=[model_checkpoint_callback],
                              batch_size=batch_size,
                              epochs=num_epochs,
                              shuffle=True)

    def save(self, save_folder="."):
        self._create_folder_if_it_doesnt_exist(save_folder)
        self._save_parameters(save_folder)
        self._save_weights(save_folder)

    def load_weights(self, weights_path):
        self.model.load_weights(weights_path)

    def reconstruct(self, images):
        latent_representations = self.encoder.predict(images)
        reconstructed_images = self.decoder.predict(latent_representations)
        return reconstructed_images, latent_representations

    @classmethod
    def load(cls, save_folder="."):
        parameters_path = os.path.join(save_folder, "parameters.pkl")
        with open(parameters_path, "rb") as f:
            parameters = pickle.load(f)
        autoencoder = AE(*parameters)
        weights_path = os.path.join(save_folder, "weights.h5")
        autoencoder.load_weights(weights_path)
        return autoencoder

    def _calculate_reconstruction_loss(self, y_target, y_predicted):
        error = y_target - y_predicted
        reconstruction_loss = K.mean(K.square(error), axis=[1, 2])
        return reconstruction_loss

    def _create_folder_if_it_doesnt_exist(self, folder):
        if not os.path.exists(folder):
            os.makedirs(folder)

    def _save_parameters(self, save_folder):
        parameters = [
            self.input_shape,
            self.conv_filters,
            self.conv_kernels,
            self.conv_strides,
            self.latent_space_dim
        ]
        save_path = os.path.join(save_folder, "parameters.pkl")
        with open(save_path, "wb") as f:
            pickle.dump(parameters, f)

    def _save_weights(self, save_folder):
        save_path = os.path.join(save_folder, "weights.h5")
        self.model.save_weights(save_path)

    def _build(self):
        self._build_encoder()
        self._build_decoder()
        self._build_autoencoder()

    # Buiding AutoEncoder

    def _build_autoencoder(self):
        model_input = self._model_input
        model_output = self.decoder(self.encoder(model_input))
        self.model = Model(model_input, model_output, name="autoencoder")

    # Buiding Decoder

    def _build_decoder(self):
        decoder_input = self._add_decoder_input()
        dense_layer = self._add_dense_layer(decoder_input)
        reshape_layer = self._add_reshape_layer(dense_layer)
        conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)
        decoder_output = self._add_decoder_output(conv_transpose_layers)
        self.decoder = Model(decoder_input, decoder_output, name="decoder")

    def _add_decoder_input(self):
        return Input(shape=self.latent_space_dim, name="decoder_input")

    def _add_dense_layer(self, decoder_input):
        num_neurons = np.prod(self._encoder_output_dim)  # [1, 2, 4] -> 8
        dense_layer = Dense(num_neurons, name="decoder_dense")(decoder_input)
        return dense_layer

    def _add_reshape_layer(self, dense_layer):
        return Reshape(self._encoder_output_dim)(dense_layer)

    def _add_conv_transpose_layers(self, x):
        """Add conv transpose blocks."""
        # loop through all the conv layers in reverse order and stop at the
        # first layer
        for layer_index in reversed(range(1, self._num_conv_layers)):
            x = self._add_conv_transpose_layer(layer_index, x)
        return x

    def _add_conv_transpose_layer(self, layer_index, x):
        layer_num = self._num_conv_layers - layer_index
        conv_transpose_layer = Conv2DTranspose(
            filters=self.conv_filters[layer_index],
            kernel_size=self.conv_kernels[layer_index],
            strides=self.conv_strides[layer_index],
            padding="same",
            name=f"decoder_conv_transpose_layer_{layer_num}"
        )
        x = conv_transpose_layer(x)
        x = ReLU(name=f"decoder_relu_{layer_num}")(x)
        x = BatchNormalization(name=f"decoder_bn_{layer_num}")(x)
        return x

    def _add_decoder_output(self, x):
        conv_transpose_layer = Conv2DTranspose(
            filters=1,
            kernel_size=self.conv_kernels[0],
            strides=self.conv_strides[0],
            padding="same",
            name=f"decoder_conv_transpose_layer_{self._num_conv_layers}"
        )
        x = conv_transpose_layer(x)
        output_layer = Activation("sigmoid", name="sigmoid_layer")(x)
        output_layer = x
        return output_layer

    # Buiding Encoder

    def _build_encoder(self):
        encoder_input = self._add_encoder_input()
        conv_layers = self._add_conv_layers(encoder_input)
        bottleneck = self._add_bottleneck(conv_layers)
        # self._encoder_output_dim = K.int_shape(conv_layers)[1:]
        self._model_input = encoder_input
        self.encoder = Model(encoder_input, bottleneck, name="encoder")
        print('Encoder Built')

    def _add_encoder_input(self):
        return Input(shape=self.input_shape, name="encoder_input")

    def _add_conv_layers(self, encoder_input):
        """Create all convolutional blocks in encoder."""
        x = encoder_input
        for layer_index in range(self._num_conv_layers):
            x = self._add_conv_layer(layer_index, x)
        return x

    def _add_conv_layer(self, layer_index, x):
        """Add a convolutional block to a graph of layers, consisting of
        conv 2d + ReLU + batch normalization.
        """
        layer_number = layer_index + 1
        conv_layer = Conv2D(
            filters=self.conv_filters[layer_index],
            kernel_size=self.conv_kernels[layer_index],
            strides=self.conv_strides[layer_index],
            padding="same",
            name=f"encoder_conv_layer_{layer_number}"
        )
        x = conv_layer(x)
        x = ReLU(name=f"encoder_relu_{layer_number}")(x)
        x = BatchNormalization(name=f"encoder_bn_{layer_number}")(x)
        return x

    def _add_bottleneck(self, x):
        """Flatten data and add bottleneck with Guassian sampling (Dense
        layer).
        """
        self._encoder_output_dim = K.int_shape(x)[1:]
        x = Flatten()(x)
        x = Dense(self.latent_space_dim, name="latent_space")(x)
        print('Bottelneck shape=', x.shape)
        return x




# Create a moving average of the ecg signals

from scipy import signal

N_avg = 30


def average_ecg(data):
    x = data.shape[0]
    y = data.shape[1]

    augmented_data = np.concatenate((data, data[:, -512 * N_avg:-512]), axis=1)
    one_positions = np.arange(0, 512 * N_avg, 512)
    window = np.zeros(512 * N_avg)
    window[one_positions] = 1

    data_avg = np.zeros((x, y))
    for i in range(6):
        intermed = signal.convolve(data[i, :], window) / N_avg
        data_avg[i] = intermed[512 * (N_avg - 1):512 * (N_avg - 1) + y]

    return data_avg



import matplotlib.pyplot as plt



def modify_shape(array, input_length):
    '''
      Modify array shape from (6, n of samples) -----> (n of heart beats, 6, 512, 1)
      n of heart beats = n of samples/512.
      512 = number of samples per hear beat.
    '''
    if array.ndim != 2:
        raise Exception("Sorry, the number of dimension of the given array must be 2.")
    if array.shape[0] != 6:
        raise Exception("Sorry, the array must contain 6 channels.")

    dim = array.shape
    print("Modify Shape func : the given array is of dim =", dim)
    n_hbeats = int(dim[1] / 512)
    array = np.stack(np.split(array, np.arange(input_length, dim[1], input_length), axis=1))
    array = array.reshape(n_hbeats, 6 * input_length, 1)
    array = array.reshape(array.shape[0], 6, 512, 1)
    return array





# Create average for training set
def to_original_dim(ecg_test):
    ecg_test = ecg_test.squeeze()
    dim = ecg_test.shape
    ecg_test_list = np.split(ecg_test, np.arange(1, dim[0]), axis=0)
    ecg_test_list = [item.squeeze() for item in ecg_test_list]

    return np.hstack(ecg_test_list)


from tensorflow import keras




LEARNING_RATE = 0.0001
BATCH_SIZE = 32
EPOCHS = 200
LATENT_SPACE = 25


def train(x_train, learning_rate, batch_size, epochs):
    autoencoder = AE(
        input_shape=(6, 512, 1),
        conv_filters=(40, 20, 20, 20, 20, 40),
        conv_kernels=([6, 40], [6, 40], [6, 40], [6, 40], [6, 40], [6, 40]),
        conv_strides=([1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2]),
        latent_space_dim=LATENT_SPACE)
    autoencoder.summary()
    print('Before Compile')
    autoencoder.compile(learning_rate)
    print('Compiled')
    history = autoencoder.train(x_train, batch_size, epochs)
    return autoencoder, history


def train_with_avg(x_train, x_train_avg, learning_rate, batch_size, epochs):
    autoencoder = AE(
        input_shape=(2, 512, 1),
        conv_filters=(40, 20, 20, 20, 20, 40),
        conv_kernels=([6, 40], [6, 40], [6, 40], [6, 40], [6, 40], [6, 40]),
        conv_strides=([1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2]),
        latent_space_dim=LATENT_SPACE)
    autoencoder.summary()
    print('Before Compile')
    autoencoder.compile(learning_rate)
    print('Compiled')
    history = autoencoder.train_with_avg(x_train, x_train_avg, batch_size, epochs)
    return autoencoder, history

from Code.ECG_Modeling.DataLoaders.PhysioNetLoader import PhyioNetLoader_MIT_NIH

loader = PhyioNetLoader_MIT_NIH(num_sets=1,num_beats=1,num_samples=512,SNR_dB= 6, random_sample=False)

num_batches = 1000


obs_non_roll,trin_non_roll = loader.GetData(num_batches)
obs,state = loader.GetRolledData(num_batches,max_roll=50)

split = int(0.8*num_batches)
obs_train = obs_non_roll[:split].unsqueeze(-1).numpy()
state_train = trin_non_roll[:split].unsqueeze(-1).numpy()

obs_test = obs[split:].unsqueeze(-1).numpy()
state_test = state[split:].unsqueeze(-1).numpy()


start_time = time.time()
autoencoder, history = train_with_avg(obs_train, state_train, LEARNING_RATE, BATCH_SIZE, EPOCHS)
print("--- Training time is %s sec ---" % (time.time() - start_time))
autoencoder.save("model")

autoencoder.model.save('netron_model.h5')

# RUN IT IF YOU WANT TO SAVE THE MODEL PARAMETERS
# autoencoder.save("model")

print(history.history.keys())

# LOSS PLOT
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

figure(figsize=(8, 6), dpi=80)
log_loss = 10 * np.log10(np.array(history.history['loss']) / (512 * 6))
plt.plot(log_loss)
# plt.plot(history.history['val_mse'])
plt.title('model loss')
plt.ylabel('10*log(loss)')
plt.xlabel('epoch')
plt.legend(['Training loss'], loc='upper left')
plt.savefig('Training_loss.png', bbox_inches='tight')
plt.show()

figure(figsize=(8, 6), dpi=80)
# plt.plot(history.history['loss'])
log_loss = 10 * np.log10(np.array(history.history['val_mean_squared_error']) / (512 * 6))
plt.plot(log_loss, color='red')
plt.title('model loss')
plt.ylabel('10*log(loss)')
plt.xlabel('epoch')
plt.legend(['Validation loss'], loc='upper left')
plt.savefig('Validation_loss.png', bbox_inches='tight')
plt.show()

"""## Load Model (Run only if the model was not trained)"""

# autoencoder = AE.load("model")

# print(history.history.keys())

"""# Ploting Results"""

signals = []
noisy = []
reconstructed_signals = []
for i in range(10):
    reconstructed = autoencoder.reconstruct(obs_test[i:i + 1, :])
    reconstructed = np.asarray(reconstructed[0])
    reconstructed = reconstructed.reshape(512 * 2, 1)
    signals.append(state_test[i:i + 1, :].reshape(512 * 2, 1))
    noisy.append(obs_test[i:i + 1, :].reshape(512 * 2, 1))
    reconstructed_signals.append(reconstructed)

for i in range(10):
    """
    fig, axis = plt.subplots(1,1,figsize=(15,3))
    axis[0].plot(signals[i])
    axis[1].plot(reconstructed_signals[i])
    axis[0].set_title('Signal')
    axis[1].set_title('Reconstructed Signal')
    """
    figure(figsize=(8, 6), dpi=80)
    plt.plot(signals[i][:512], 'b')
    plt.plot(reconstructed_signals[i][:512], 'g')
    plt.plot(noisy[i][:512],'r',alpha = 0.4)
    plt.title('Averaged and reconstructed signal after VAC')
    plt.xlabel('Samples')
    plt.legend(['Ground Truth', 'Reconstructed','Noisy signal'], loc='upper left')
    plt.savefig('test_10_' + str(i) + '.png', bbox_inches='tight')
    plt.show()

"""# Compute performance

"""


# Compare performances

# test_reconstructions, ecg_test ----> ecg_test_avg

def performance(signal, estimation):
    if signal.shape != estimation.shape:
        raise Exception("The given array dimensions must be the same.")

    signal = signal.squeeze()
    estimation = estimation.squeeze()

    diff = signal - estimation
    epsilon = np.sum(np.linalg.norm(diff, axis=2) ** 2, axis=0) / np.sum(np.linalg.norm(signal, axis=2) ** 2, axis=0)
    epsilon = np.squeeze(epsilon)
    epsilon = np.sum(epsilon) / signal.shape[1]

    return epsilon


test_reconstructions = autoencoder.reconstruct(obs_test)
test_reconstructions = np.asarray(test_reconstructions[0])

error_init = performance(state_test, obs_test)
error_model = performance(state_test, test_reconstructions)

print('test_reconstructions shape=', test_reconstructions.shape)
print('avg_ecg shape=', test_reconstructions.shape)
print('ecg_test shape=', test_reconstructions.shape)

print('error_init =', 10*np.log10(error_init))
print('error_model =', 10*np.log10(error_model),'[dB]')

"""# Save Files

"""


